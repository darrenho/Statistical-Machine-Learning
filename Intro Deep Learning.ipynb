{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "If using GPUs:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is best to divide the data set into minibatches (see Stochastic Gradient Descent).\n",
      "\n",
      "We encourage you to store the dataset into shared variables and access it based on the minibatch\n",
      "index, given a fixed and known batch size. The reason behind shared variables is related to\n",
      "using the GPU. There is a large overhead when copying data into the GPU memory. If you\n",
      "would copy data on request ( each minibatch individually when needed) as the code will do if\n",
      "you do not use shared variables, due to this overhead, the GPU code will not be much faster\n",
      "then the CPU code (maybe even slower). If you have your data in Theano shared variables\n",
      "though, you give Theano the possibility to copy the entire data on the GPU in a single call\n",
      "when the shared variables are constructed. Afterwards the GPU can access any minibatch by\n",
      "taking a slice from this shared variables, without needing to copy any information from the\n",
      "CPU memory and therefore bypassing the overhead. Because the datapoints and their labels\n",
      "are usually of different nature (labels are usually integers while datapoints are real numbers) we\n",
      "suggest to use different variables for labes and data. Also we recomand using different variables\n",
      "for the training set, validation set and testing set to make the code more readable (resulting in 6\n",
      "different shared variables).\n",
      "\n",
      "Since now the data is in one variable, and a minibatch is defined as a slice of that variable,\n",
      "it comes more natural to define a minibatch by indicating its index and its size. In our setup\n",
      "the batch size stays constant through out the execution of the code, therefore a function will\n",
      "actually require only the index to identify on which datapoints to work. \n",
      "\n",
      "The code below shows how to store your data and how to access a minibatch:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def shared_dataset(data_xy):\n",
      "    \"\"\" Function that loads the dataset into shared variables\n",
      "        The reason we store our dataset in shared variables is to allow\n",
      "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
      "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
      "        is needed (the default behaviour if the data is not in a shared\n",
      "        variable) would lead to a large decrease in performance.\n",
      "        \"\"\"\n",
      "    data_x, data_y = data_xy\n",
      "    shared_x = theano.shared(numpy.asarray(data_x, dtype=theano.config.floatX))\n",
      "    shared_y = theano.shared(numpy.asarray(data_y, dtype=theano.config.floatX))\n",
      "        # When storing data on the GPU it has to be stored as floats\n",
      "        # therefore we will store the labels as \u2018\u2018floatX\u2018\u2018 as well\n",
      "        # (\u2018\u2018shared_y\u2018\u2018 does exactly that). But during our computations\n",
      "        # we need them as ints (we use labels as index, and if they are\n",
      "        # floats it doesn\u2019t make sense) therefore instead of returning\n",
      "        # \u2018\u2018shared_y\u2018\u2018 we will have to cast it to int. This little hack\n",
      "        # lets us get around this issue\n",
      "    return shared_x, T.cast(shared_y, \"int32\")\n",
      "\n",
      "\n",
      "test_set_x, test_set_y = shared_dataset(test_set)\n",
      "valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
      "train_set_x, train_set_y = shared_dataset(train_set)\n",
      "\n",
      "\n",
      "\n",
      "batch_size = 500 # size of the minibatch\n",
      "\n",
      "\n",
      "# accessing the third minibatch of the training set\n",
      "data = train_set_x[2 * 500: 3 * 500]\n",
      "label = train_set_y[2 * 500: 3 * 500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'test_set' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-791bb8cfca4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtest_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mvalid_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_set_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtrain_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'test_set' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The data has to be stored as floats on the GPU ( the right dtype for storing on the GPU is given by\n",
      "theano.config.floatX). To get around this shortcomming for the labels, we store them as float, and\n",
      "then cast it to int."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note: If you are running your code on the GPU and the dataset you are using is too large to fit in memory\n",
      "the code will crash. In such a case you should store the data in a shared variable. You can however store a\n",
      "sufficiently small chunk of your data (several minibatches) in a shared variable and use that during training.\n",
      "Once you got through the chunk, update the values it stores. This way you minimize the number of data\n",
      "transfers between CPU memory and GPU memory."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Autoencoders and the Denoising Autoencoder (dA)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The Denoising Autoencoder (dA) is an extension of a classical autoencoder and it was introduced as a\n",
      "building block for deep networks in [Vincent08]. We will start the tutorial with a short discussion on\n",
      "Autoencoders."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/ae.jpg\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An autoencoder takes an input $x \\in [0, 1]^d$\n",
      "and first maps it (with an $encoder$) to a hidden representation $y \\in [0, 1]^{d'}$ through a deterministic mapping,\n",
      "\n",
      "e.g.:       \n",
      "\n",
      "y = s(Wx+ b)\n",
      "\n",
      "where s is a non-linearity such as the sigmoid. The latent representation y, or code, is then mapped back\n",
      "(with a $decoder$) into a reconstruction z of same shape as x through a similar transformation, \n",
      "\n",
      "e.g.:               \n",
      "\n",
      "z = s(W'y + b')\n",
      "\n",
      "where \u2018 does not indicate transpose, and z should be seen as a prediction of x, given the code y. The weight\n",
      "matrix W' of the reverse mapping may be optionally constrained by $W = W'$ , which is an instance of\n",
      "tied weights. The parameters of this model (namely W, b, b' and, if one doesn\u2019t use tied weights, also\n",
      "W') are optimized such that the average reconstruction error is minimized. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The reconstruction error can be measured in many ways, depending on the appropriate distributional assumptions on the input given the\n",
      "code, \n",
      "\n",
      "e.g., using the traditional $squared$ $error$ $L(x, z) = ||x - z||^2$, \n",
      "\n",
      "or if the input is interpreted as either bit vectors or vectors of bit probabilities by the reconstruction $cross-entropy$. Thus, the mean squared error criterion can be generalized to the negative log-likelihood of the reconstruction, given the encoding, z(x):\n",
      "\n",
      "RE = - log $P(x|z(x))$\n",
      "\n",
      "and when the inputs $x_i$ are either binary or considered to be binomial probabilities, then the loss function would be\n",
      "\n",
      "\n",
      "-log $P(x|z(x)$ = $L_H(x, z) = -\\sum_{k=1}^d \\left[ x_k log (z_k) + (1 - x_k) log(1 - z_k)\\right]$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/recon.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The hope is that the code y is a distributed representation that captures the coordinates along the main factors\n",
      "of variation in the data (similarly to how the projection on principal components captures the main factors\n",
      "of variation in the data). Because y is viewed as a lossy compression of x, it cannot be a good compression\n",
      "(with small loss) for all x, so learning drives it to be one that is a good compression in particular for training\n",
      "examples, and hopefully for others as well, but not for arbitrary inputs. That is the sense in which an autoencoder\n",
      "generalizes: it gives low reconstruction error to test examples from the same distribution as the\n",
      "training examples, but generally high reconstruction error to uniformly chosen configurations of the input\n",
      "vector.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If there is one linear hidden layer (the code) and the mean squared error criterion is used to train the network,\n",
      "then the k hidden units learn to project the input in the span of the first k principal components of the data.\n",
      "If the hidden layer is non-linear, the auto-encoder behaves differently from PCA, with the ability to capture\n",
      "multi-modal aspects of the input distribution. The departure from PCA becomes even more important when\n",
      "we consider stacking multiple encoders (and their corresponding decoders) when building a deep autoencoder\n",
      "[Hinton06]."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/ae2.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "let's build one!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to implement an auto-encoder using Theano, in the form of a class, that could be afterwards used\n",
      "in constructing a stacked autoencoder. The first step is to create shared variables for the parameters of the\n",
      "autoencoder (W, b and b', since we are using tied weights in this tutorial ):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class AutoEncoder(object):\n",
      "    \n",
      "    def __init__(self, numpy_rng, input=None, n_visible=784, n_hidden=500,\n",
      "            W=None, bhid=None, bvis=None):\n",
      "        \"\"\"\n",
      "        :type numpy_rng: numpy.random.RandomState\n",
      "        :param numpy_rng: number random generator used to generate weights\n",
      "        \n",
      "        :type input: theano.tensor.TensorType\n",
      "        :paran input: a symbolic description of the input or None for standalone dA\n",
      "        \n",
      "        :type n_visible: int\n",
      "        :param n_visible: number of visible units\n",
      "        \n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "        \n",
      "        :type W: theano.tensor.TensorType\n",
      "        :param W: Theano variable pointing to a set of weights that should be \n",
      "            shared belong the dA and another architecture; if dA should\n",
      "            be standalone set this to None\n",
      "        \n",
      "        :type bhid: theano.tensor.TensorType\n",
      "        :param bhid: Theano variable pointing to a set of biases values (for\n",
      "            hidden units) that should be shared belong dA and another\n",
      "            architecture; if dA should be standalone set this to None\n",
      "        \n",
      "        :type bvis: theano.tensor.TensorType\n",
      "        :param bvis: Theano variable pointing to a set of biases values (for\n",
      "            visible units) that should be shared belong dA and another\n",
      "            architecture; if dA should be standalone set this to None\n",
      "        \"\"\"\n",
      "    \n",
      "    self.n_visible = n_visible\n",
      "    self.n_hidden = n_hidden\n",
      "\n",
      "    # note : W\u2019 was written as \u2018W_prime\u2018 and b\u2019 as \u2018b_prime\u2018\n",
      "    if not W:\n",
      "        # W is initialized with \u2018initial_W\u2018 which is uniformely sampled\n",
      "        # from -4*sqrt(6./(n_visible+n_hidden)) and 4*sqrt(6./(n_hidden+n_visible))\n",
      "        # the output of uniform if converted using asarray to dtype\n",
      "        # theano.config.floatX so that the code is runable on GPU\n",
      "   \n",
      "        initial_W = numpy.asarray(numpy_rng.uniform(\n",
      "                low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
      "                high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
      "                size=(n_visible, n_hidden)), dtype=theano.config.floatX)\n",
      "        \n",
      "        W = theano.shared(value=initial_W, name=\"W\")\n",
      "\n",
      "    if not bvis:\n",
      "        bvis = theano.shared(value=numpy.zeros(n_visible,\n",
      "                                dtype=theano.config.floatX), name=\"bvis\")\n",
      "\n",
      "    if not bhid:\n",
      "        bhid = theano.shared(value=numpy.zeros(n_hidden,\n",
      "                dtype=theano.config.floatX), name=\"bhid\")\n",
      "    \n",
      "    self.W = W\n",
      "    # b corresponds to the bias of the hidden\n",
      "    self.b = bhid\n",
      "    # b_prime corresponds to the bias of the visible\n",
      "    self.b_prime = bvis\n",
      "    # tied weights, therefore W_prime is W transpose\n",
      "    self.W_prime = self.W.T\n",
      "    # if no input is given, generate a variable representing the input\n",
      "\n",
      "    if input == None:\n",
      "        # we use a matrix because we expect a minibatch of several examples,\n",
      "        # each example being a row\n",
      "        self.x = T.dmatrix(name=\"input\")\n",
      "    else:\n",
      "        self.x = input\n",
      "\n",
      "    self.params = [self.W, self.b, self.b_prime]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'n_visible' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-be56cf136b60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def __init__(self, numpy_rng, input=None, n_visible=784, n_hidden=500,\n\u001b[1;32m      4\u001b[0m             W=None, bhid=None, bvis=None):\n\u001b[1;32m      5\u001b[0m         \"\"\"\n",
        "\u001b[0;32m<ipython-input-1-be56cf136b60>\u001b[0m in \u001b[0;36mAutoEncoder\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \"\"\"\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_visible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_visible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'n_visible' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that we pass the symbolic input to the autoencoder as a parameter. This is such that later we can\n",
      "concatenate layers of autoencoders to form a deep network: the symbolic output (the y above) of the k-th\n",
      "layer will be the symbolic input of the (k+1)-th."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can express the computation of the latent representation and of the reconstructed signal:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_hidden_values(self, input):\n",
      "    \"\"\" Computes the values of the hidden layer \"\"\"\n",
      "    return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
      "\n",
      "\n",
      "def get_reconstructed_input(self, hidden):\n",
      "    \"\"\" Computes the reconstructed input given the values of the hidden layer \"\"\"\n",
      "    return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And using these function we can compute the cost and the updates of one stochastic gradient descent step :"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_cost_updates(self, learning_rate):\n",
      "    \"\"\" This function computes the cost and the updates for one trainng\n",
      "        step \n",
      "        \"\"\"\n",
      "\n",
      "    y = self.get_hidden_values(self.x)\n",
      "    z = self.get_reconstructed_input(y)\n",
      "\n",
      "    # note : we sum over the size of a datapoint; if we are using minibatches,\n",
      "    # L will be a vector, with one entry per example in minibatch\n",
      "    L = -T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1)\n",
      "\n",
      "    # note : L is now a vector, where each element is the cross-entropy cost\n",
      "    # of the reconstruction of the corresponding example of the\n",
      "    # minibatch. We need to compute the average of all these to get\n",
      "    # the cost of the minibatch\n",
      "    cost = T.mean(L)\n",
      "\n",
      "    # compute the gradients of the cost of the \u2018dA\u2018 with respect\n",
      "    # to its parameters\n",
      "    gparams = T.grad(cost, self.params)\n",
      "\n",
      "    # generate the list of updates\n",
      "    updates = []\n",
      "    for param, gparam in zip(self.params, gparams):\n",
      "            updates.append((param, param - learning_rate * gparam))\n",
      "\n",
      "    return (cost, updates)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now define a function that applied iteratively will update the parameters W, b and b_prime such\n",
      "that the reconstruction cost is approximately minimized."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "autoencoder = AutoEncoder(numpy_rng=numpy.random.RandomState(1234), n_visible=784, n_hidden=\n",
      "\n",
      "cost, updates = autoencoder.get_cost_updates(learning_rate=0.1)\n",
      "\n",
      "train = theano.function([x], cost, updates=updates)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-17-ec35f1ef5aed>, line 3)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-ec35f1ef5aed>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    train = theano.function([x], cost, updates=updates)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One serious potential issue with auto-encoders is that if there is no other constraint besides minimizing the\n",
      "reconstruction error, then an auto-encoder with $n$ inputs and an encoding of dimension at least $n$ could\n",
      "potentially just learn the identity function, for which many encodings would be useless (e.g., just copying\n",
      "the input), i.e., the autoencoder would not differentiate test examples (from the training distribution) from\n",
      "other input configurations. Surprisingly, experiments reported in [Bengio07] nonetheless suggest that in\n",
      "practice, when trained with stochastic gradient descent, non-linear auto-encoders with more hidden units\n",
      "than inputs (called overcomplete) yield useful representations (in the sense of classification error measured\n",
      "on a network taking this representation in input). A simple explanation is based on the observation that\n",
      "stochastic gradient descent with early stopping is similar to an L2 regularization of the parameters. To\n",
      "achieve perfect reconstruction of continuous inputs, a one-hidden layer auto-encoder with non-linear hidden units (exactly like in the above code) needs very small weights in the first (encoding) layer (to bring the\n",
      "non-linearity of the hidden units in their linear regime) and very large weights in the second (decoding)\n",
      "layer. With binary inputs, very large weights are also needed to completely minimize the reconstruction\n",
      "error. Since the implicit or explicit regularization makes it difficult to reach large-weight solutions, the\n",
      "optimization algorithm finds encodings which only work well for examples similar to those in the training\n",
      "set, which is what we want. It means that the representation is exploiting statistical regularities present in\n",
      "the training set, rather than learning to replicate the identity function.\n",
      "\n",
      "There are different ways that an auto-encoder with more hidden units than inputs could be prevented from\n",
      "learning the identity, and still capture something useful about the input in its hidden representation. One is\n",
      "the addition of sparsity (forcing many of the hidden units to be zero or near-zero), and it has been exploited\n",
      "very successfully by many [Ranzato07] [Lee08]. Another is to add randomness in the transformation from\n",
      "input to reconstruction. This is exploited in Restricted Boltzmann Machines (discussed later in Restricted\n",
      "Boltzmann Machines (RBM)), as well as in Denoising Auto-Encoders, discussed below."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Now... let's add a little noise: Denoising Autoencoders (dA)!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, the clean input $x \\in [0,1]^d$, is partially (stochastically) corrupted, yielding $\\tilde{x} \\sim q_D(\\tilde{x} | x)$\n",
      "\n",
      "Then, $\\tilde{x}$ is mapped to a hidden representation $y=f_{\\theta}(\\tilde{x})$.\n",
      "\n",
      "From y we reconstruct a $z= g_{\\theta'}(y)$ and then train the parameters to minimize the \"reconstruction error.\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/da.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So... we choose a fixed proportion $\\nu$ of components of $\\textbf{x}$ at random.\n",
      "\n",
      "Reset their values to 0.\n",
      "\n",
      "Can be viewed as replacing a component considered missing by a default value.\n",
      "\n",
      "*note: other corruption processes can be considered.\n",
      "\n",
      "we learn the first mapping $f_{\\theta}$ by training as a denoising autoencoder."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/corrupt.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use standard sigmoid network layers:\n",
      "    \n",
      "$\\textbf{y} = f_{\\theta}(\\tilde{x}) = $ sigmoid $(W \\tilde{x} + \\textbf{b})$\n",
      "\n",
      "$g_{\\theta'}(\\textbf{y}) =$ sigmoid $(W'\\textbf{y} + \\textbf{b}')$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "With the MNIST digits data set as an example:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/sae.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$Just$ $for$ $completeness$:\n",
      "\n",
      "Denoising using autoencoders were actually introduced a loooong time ago (LeCun, 1987; Gallinari et al., 1987), as an alternative to Hopfield networks (Hopfiled,1982)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "well... what's the difference here, between Autoencoders and Denoising Autoencoders?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On the left we see the filters reconstructed from the corrupted images while on the right are the filters reconstructed from the uncorrupted images."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/avsda.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "the images on the left seem to give a $much$ better representation of the digits!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "now... let's take it deep!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Learn the first mapping $f_{\\theta}$ by training as a denoising autoencoder.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/da.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We then remove the scaffolding and use $f_{\\theta}$ directly on the input, yielding the higher-level representation:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/high.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We then learn the next level mapping $f_{\\theta}^{(2)}$ by training denoising autoencoder on current level representation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/next.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and...of course, we simply iterate to initialize the subsequent layers!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/layers.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Let's recap:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\cdot$ Initial deep mapping was learned in an $unsupervised$ way.\n",
      "\n",
      "$\\cdot$ $initialization$ for a supervised task --> output layer gets added\n",
      "\n",
      "and we can do global fine tuning by gradient descent on supervised criterion!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/super.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Perspectives on denoising Autoencoder:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Denoising autoencoder can be seen as a way to $learn$ $a$ $manifold$:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\cdot$ Suppose training data $\\textbf{x}$ concentrate near a low dimensional manifold\n",
      "\n",
      "$\\cdot$ Corrupted examples $\\tilde{x}$ are obtained by applying a corruption process, $q_{\\mathcal{D}}(\\tilde{X} | X)$ and will $lie$ $farther$ $from$ $the$ $manifold$\n",
      "\n",
      "$\\cdot$ the model learns with $p(X|\\tilde{X})$ to \"project them back\" onto the manifold\n",
      "\n",
      "$\\cdot$ intermediate representation Y can be interpreted as a coordinate system for points on the manifold"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/mani.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "References can be found, here:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bengio, Y., Lamblin, P., Popovici, D., & Larochelle, H. (2007). Greedy\n",
      "layer-wise training of deep networks. Advances in Neural Information\n",
      "Processing Systems 19 (pp. 153\u2013160). MIT Press.\n",
      "\n",
      "Gallinari, P., LeCun, Y., Thiria, S., & Fogelman-Soulie, F. (1987). Memoires\n",
      "associatives distribuees. Proceedings of COGNITIVA 87. Paris, La Villette.\n",
      "\n",
      "Hinton, G. E., Osindero, S., & Teh, Y. (2006). A fast learning algorithm for\n",
      "deep belief nets. Neural Computation, 18, 1527\u20131554.\n",
      "\n",
      "Hopfield, J. (1982). Neural networks and physical systems with emergent\n",
      "collective computational abilities. Proceedings of the National Academy of\n",
      "Sciences, USA, 79.\n",
      "\n",
      "LeCun, Y. (1987). Mod`eles connexionistes de l\u2019apprentissage. Doctoral\n",
      "dissertation, Universit\u00b4e de Paris VI.\n",
      "\n",
      "Ranzato, M., Poultney, C., Chopra, S., & LeCun, Y. (2007). Efficient learning\n",
      "of sparse representations with an energy-based model. Advances in Neural\n",
      "Information Processing Systems (NIPS 2006). MIT Press.\n",
      "\n",
      "Rumelhart, D., Hinton, G., & Williams, R. (1986). Learning representations by\n",
      "back-propagating errors. Nature, 323, 533\u2013536."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "So... can we build one?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      " This tutorial introduces denoising auto-encoders (dA) using Theano.\n",
      "\n",
      " Denoising autoencoders are the building blocks for SdA.\n",
      " They are based on auto-encoders as the ones used in Bengio et al. 2007.\n",
      " An autoencoder takes an input x and first maps it to a hidden representation\n",
      " y = f_{\\theta}(x) = s(Wx+b), parameterized by \\theta={W,b}. The resulting\n",
      " latent representation y is then mapped back to a \"reconstructed\" vector\n",
      " z \\in [0,1]^d in input space z = g_{\\theta'}(y) = s(W'y + b').  The weight\n",
      " matrix W' can optionally be constrained such that W' = W^T, in which case\n",
      " the autoencoder is said to have tied weights. The network is trained such\n",
      " that to minimize the reconstruction error (the error between x and z).\n",
      "\n",
      " For the denosing autoencoder, during training, first x is corrupted into\n",
      " \\tilde{x}, where \\tilde{x} is a partially destroyed version of x by means\n",
      " of a stochastic mapping. Afterwards y is computed as before (using\n",
      " \\tilde{x}), y = s(W\\tilde{x} + b) and z as s(W'y + b'). The reconstruction\n",
      " error is now measured between z and the uncorrupted input x, which is\n",
      " computed as the cross-entropy :\n",
      "      - \\sum_{k=1}^d[ x_k \\log z_k + (1-x_k) \\log( 1-z_k)]\n",
      "\n",
      "\n",
      " References :\n",
      "   - P. Vincent, H. Larochelle, Y. Bengio, P.A. Manzagol: Extracting and\n",
      "   Composing Robust Features with Denoising Autoencoders, ICML'08, 1096-1103,\n",
      "   2008\n",
      "   - Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle: Greedy Layer-Wise\n",
      "   Training of Deep Networks, Advances in Neural Information Processing\n",
      "   Systems 19, 2007\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import cPickle\n",
      "import gzip\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "from theano.tensor.shared_randomstreams import RandomStreams\n",
      "\n",
      "from logistic_sgd import load_data\n",
      "from utils import tile_raster_images\n",
      "\n",
      "import PIL.Image\n",
      "\n",
      "\n",
      "class dA(object):\n",
      "    \"\"\"Denoising Auto-Encoder class (dA)\n",
      "\n",
      "    A denoising autoencoders tries to reconstruct the input from a corrupted\n",
      "    version of it by projecting it first in a latent space and reprojecting\n",
      "    it afterwards back in the input space. Please refer to Vincent et al.,2008\n",
      "    for more details. If x is the input then equation (1) computes a partially\n",
      "    destroyed version of x by means of a stochastic mapping q_D. Equation (2)\n",
      "    computes the projection of the input into the latent space. Equation (3)\n",
      "    computes the reconstruction of the input, while equation (4) computes the\n",
      "    reconstruction error.\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        \\tilde{x} ~ q_D(\\tilde{x}|x)                                     (1)\n",
      "\n",
      "        y = s(W \\tilde{x} + b)                                           (2)\n",
      "\n",
      "        x = s(W' y  + b')                                                (3)\n",
      "\n",
      "        L(x,z) = -sum_{k=1}^d [x_k \\log z_k + (1-x_k) \\log( 1-z_k)]      (4)\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, numpy_rng, theano_rng=None, input=None,\n",
      "                 n_visible=784, n_hidden=500,\n",
      "                 W=None, bhid=None, bvis=None):\n",
      "        \"\"\"\n",
      "        Initialize the dA class by specifying the number of visible units (the\n",
      "        dimension d of the input ), the number of hidden units ( the dimension\n",
      "        d' of the latent or hidden space ) and the corruption level. The\n",
      "        constructor also receives symbolic variables for the input, weights and\n",
      "        bias. Such a symbolic variables are useful when, for example the input\n",
      "        is the result of some computations, or when weights are shared between\n",
      "        the dA and an MLP layer. When dealing with SdAs this always happens,\n",
      "        the dA on layer 2 gets as input the output of the dA on layer 1,\n",
      "        and the weights of the dA are used in the second stage of training\n",
      "        to construct an MLP.\n",
      "\n",
      "        :type numpy_rng: numpy.random.RandomState\n",
      "        :param numpy_rng: number random generator used to generate weights\n",
      "\n",
      "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
      "        :param theano_rng: Theano random generator; if None is given one is\n",
      "                     generated based on a seed drawn from `rng`\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: a symbolic description of the input or None for\n",
      "                      standalone dA\n",
      "\n",
      "        :type n_visible: int\n",
      "        :param n_visible: number of visible units\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden:  number of hidden units\n",
      "\n",
      "        :type W: theano.tensor.TensorType\n",
      "        :param W: Theano variable pointing to a set of weights that should be\n",
      "                  shared belong the dA and another architecture; if dA should\n",
      "                  be standalone set this to None\n",
      "\n",
      "        :type bhid: theano.tensor.TensorType\n",
      "        :param bhid: Theano variable pointing to a set of biases values (for\n",
      "                     hidden units) that should be shared belong dA and another\n",
      "                     architecture; if dA should be standalone set this to None\n",
      "\n",
      "        :type bvis: theano.tensor.TensorType\n",
      "        :param bvis: Theano variable pointing to a set of biases values (for\n",
      "                     visible units) that should be shared belong dA and another\n",
      "                     architecture; if dA should be standalone set this to None\n",
      "\n",
      "\n",
      "        \"\"\"\n",
      "        self.n_visible = n_visible\n",
      "        self.n_hidden = n_hidden\n",
      "\n",
      "        # create a Theano random generator that gives symbolic random values\n",
      "        if not theano_rng:\n",
      "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
      "\n",
      "        # note : W' was written as `W_prime` and b' as `b_prime`\n",
      "        if not W:\n",
      "            # W is initialized with `initial_W` which is uniformely sampled\n",
      "            # from -4*sqrt(6./(n_visible+n_hidden)) and\n",
      "            # 4*sqrt(6./(n_hidden+n_visible))the output of uniform if\n",
      "            # converted using asarray to dtype\n",
      "            # theano.config.floatX so that the code is runable on GPU\n",
      "            initial_W = numpy.asarray(numpy_rng.uniform(\n",
      "                      low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
      "                      high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
      "                      size=(n_visible, n_hidden)), dtype=theano.config.floatX)\n",
      "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
      "\n",
      "        if not bvis:\n",
      "            bvis = theano.shared(value=numpy.zeros(n_visible,\n",
      "                                         dtype=theano.config.floatX),\n",
      "                                 borrow=True)\n",
      "\n",
      "        if not bhid:\n",
      "            bhid = theano.shared(value=numpy.zeros(n_hidden,\n",
      "                                                   dtype=theano.config.floatX),\n",
      "                                 name='b',\n",
      "                                 borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        # b corresponds to the bias of the hidden\n",
      "        self.b = bhid\n",
      "        # b_prime corresponds to the bias of the visible\n",
      "        self.b_prime = bvis\n",
      "        # tied weights, therefore W_prime is W transpose\n",
      "        self.W_prime = self.W.T\n",
      "        self.theano_rng = theano_rng\n",
      "        # if no input is given, generate a variable representing the input\n",
      "        if input == None:\n",
      "            # we use a matrix because we expect a minibatch of several\n",
      "            # examples, each example being a row\n",
      "            self.x = T.dmatrix(name='input')\n",
      "        else:\n",
      "            self.x = input\n",
      "\n",
      "        self.params = [self.W, self.b, self.b_prime]\n",
      "\n",
      "    def get_corrupted_input(self, input, corruption_level):\n",
      "        \"\"\"This function keeps ``1-corruption_level`` entries of the inputs the\n",
      "        same and zero-out randomly selected subset of size ``coruption_level``\n",
      "        Note : first argument of theano.rng.binomial is the shape(size) of\n",
      "               random numbers that it should produce\n",
      "               second argument is the number of trials\n",
      "               third argument is the probability of success of any trial\n",
      "\n",
      "                this will produce an array of 0s and 1s where 1 has a\n",
      "                probability of 1 - ``corruption_level`` and 0 with\n",
      "                ``corruption_level``\n",
      "\n",
      "                The binomial function return int64 data type by\n",
      "                default.  int64 multiplicated by the input\n",
      "                type(floatX) always return float64.  To keep all data\n",
      "                in floatX when floatX is float32, we set the dtype of\n",
      "                the binomial to floatX. As in our case the value of\n",
      "                the binomial is always 0 or 1, this don't change the\n",
      "                result. This is needed to allow the gpu to work\n",
      "                correctly as it only support float32 for now.\n",
      "\n",
      "        \"\"\"\n",
      "        return  self.theano_rng.binomial(size=input.shape, n=1,\n",
      "                                         p=1 - corruption_level,\n",
      "                                         dtype=theano.config.floatX) * input\n",
      "\n",
      "    def get_hidden_values(self, input):\n",
      "        \"\"\" Computes the values of the hidden layer \"\"\"\n",
      "        return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
      "\n",
      "    def get_reconstructed_input(self, hidden):\n",
      "        \"\"\"Computes the reconstructed input given the values of the\n",
      "        hidden layer\n",
      "\n",
      "        \"\"\"\n",
      "        return  T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n",
      "\n",
      "    def get_cost_updates(self, corruption_level, learning_rate):\n",
      "        \"\"\" This function computes the cost and the updates for one trainng\n",
      "        step of the dA \"\"\"\n",
      "\n",
      "        tilde_x = self.get_corrupted_input(self.x, corruption_level)\n",
      "        y = self.get_hidden_values(tilde_x)\n",
      "        z = self.get_reconstructed_input(y)\n",
      "        # note : we sum over the size of a datapoint; if we are using\n",
      "        #        minibatches, L will be a vector, with one entry per\n",
      "        #        example in minibatch\n",
      "        L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1)\n",
      "        # note : L is now a vector, where each element is the\n",
      "        #        cross-entropy cost of the reconstruction of the\n",
      "        #        corresponding example of the minibatch. We need to\n",
      "        #        compute the average of all these to get the cost of\n",
      "        #        the minibatch\n",
      "        cost = T.mean(L)\n",
      "\n",
      "        # compute the gradients of the cost of the `dA` with respect\n",
      "        # to its parameters\n",
      "        gparams = T.grad(cost, self.params)\n",
      "        # generate the list of updates\n",
      "        updates = []\n",
      "        for param, gparam in zip(self.params, gparams):\n",
      "            updates.append((param, param - learning_rate * gparam))\n",
      "\n",
      "        return (cost, updates)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_dA(learning_rate=0.1, training_epochs=3,\n",
      "            dataset='mnist.pkl.gz',\n",
      "            batch_size=20, output_folder='dA_plots'):\n",
      "\n",
      "    \"\"\"\n",
      "    This demo is tested on MNIST\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used for training the DeNosing\n",
      "                          AutoEncoder\n",
      "\n",
      "    :type training_epochs: int\n",
      "    :param training_epochs: number of epochs used for training\n",
      "\n",
      "    :type dataset: string\n",
      "    :param dataset: path to the picked dataset\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    \n",
      "    import cPickle, numpy\n",
      "    f=open(\"mnist.pkl\", \"rb\")\n",
      "    train_set, valid_set, test_set = cPickle.load(f)\n",
      "    f.close()\n",
      "\n",
      "    def shared_dataset(data_xy, borrow=True):\n",
      "        \"\"\"Function that loads the dataset into shared variables\n",
      "        The reason we store our dataset in shared variables is to allow\n",
      "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
      "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
      "        is needed (the default behaviour if the data is not in a shared\n",
      "        variable) would lead to a large decrease in performance.\n",
      "        \"\"\"\n",
      "        data_x, data_y = data_xy\n",
      "        shared_x = theano.shared(numpy.asarray(data_x,\n",
      "                                               dtype=theano.config.floatX),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(numpy.asarray(data_y,\n",
      "                                               dtype=theano.config.floatX),\n",
      "                                 borrow=borrow)\n",
      "        # When storing data on the GPU it has to be stored as floats\n",
      "        # therefore we will store the labels as ``floatX`` as well\n",
      "        # (``shared_y`` does exactly that). But during our computations\n",
      "        # we need them as ints (we use labels as index, and if they are\n",
      "        # floats it doesn't make sense) therefore instead of returning\n",
      "        # ``shared_y`` we will have to cast it to int. This little hack\n",
      "        # lets ous get around this issue\n",
      "        return shared_x, T.cast(shared_y, 'int32')\n",
      "\n",
      "    train_set_x, train_set_y = shared_dataset(train_set)\n",
      "    \n",
      "    \n",
      "    #datasets = load_data(dataset)\n",
      "    #train_set_x, train_set_y = datasets[0]\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()    # index to a [mini]batch\n",
      "    x = T.matrix('x')  # the data is presented as rasterized images\n",
      "\n",
      "    if not os.path.isdir(output_folder):\n",
      "        os.makedirs(output_folder)\n",
      "    os.chdir(output_folder)\n",
      "    ####################################\n",
      "    # BUILDING THE MODEL NO CORRUPTION #\n",
      "    ####################################\n",
      "\n",
      "    rng = numpy.random.RandomState(123)\n",
      "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
      "\n",
      "    da = dA(numpy_rng=rng, theano_rng=theano_rng, input=x,\n",
      "            n_visible=28 * 28, n_hidden=500)\n",
      "\n",
      "    cost, updates = da.get_cost_updates(corruption_level=0.,\n",
      "                                        learning_rate=learning_rate)\n",
      "\n",
      "    train_da = theano.function([index], cost, updates=updates,\n",
      "         givens={x: train_set_x[index * batch_size:\n",
      "                                (index + 1) * batch_size]})\n",
      "\n",
      "    start_time = time.clock()\n",
      "\n",
      "    ############\n",
      "    # TRAINING #\n",
      "    ############\n",
      "\n",
      "    # go through training epochs\n",
      "    for epoch in xrange(training_epochs):\n",
      "        # go through trainng set\n",
      "        c = []\n",
      "        for batch_index in xrange(n_train_batches):\n",
      "            c.append(train_da(batch_index))\n",
      "\n",
      "        print 'Training epoch %d, cost ' % epoch, numpy.mean(c)\n",
      "\n",
      "    end_time = time.clock()\n",
      "\n",
      "    training_time = (end_time - start_time)\n",
      "\n",
      "    print >> sys.stderr, ('The no corruption code for file ' +\n",
      "                          os.path.split(__file__)[1] +\n",
      "                          ' ran for %.2fm' % ((training_time) / 60.))\n",
      "    image = PIL.Image.fromarray(\n",
      "        tile_raster_images(X=da.W.get_value(borrow=True).T,\n",
      "                           img_shape=(28, 28), tile_shape=(10, 10),\n",
      "                           tile_spacing=(1, 1)))\n",
      "    image.save('filters_corruption_0.png')\n",
      "\n",
      "    #####################################\n",
      "    # BUILDING THE MODEL CORRUPTION 30% #\n",
      "    #####################################\n",
      "\n",
      "    rng = numpy.random.RandomState(123)\n",
      "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
      "\n",
      "    da = dA(numpy_rng=rng, theano_rng=theano_rng, input=x,\n",
      "            n_visible=28 * 28, n_hidden=500)\n",
      "\n",
      "    cost, updates = da.get_cost_updates(corruption_level=0.3,\n",
      "                                        learning_rate=learning_rate)\n",
      "\n",
      "    train_da = theano.function([index], cost, updates=updates,\n",
      "         givens={x: train_set_x[index * batch_size:\n",
      "                                  (index + 1) * batch_size]})\n",
      "\n",
      "    start_time = time.clock()\n",
      "\n",
      "    ############\n",
      "    # TRAINING #\n",
      "    ############\n",
      "\n",
      "    # go through training epochs\n",
      "    for epoch in xrange(training_epochs):\n",
      "        # go through trainng set\n",
      "        c = []\n",
      "        for batch_index in xrange(n_train_batches):\n",
      "            c.append(train_da(batch_index))\n",
      "\n",
      "        print 'Training epoch %d, cost ' % epoch, numpy.mean(c)\n",
      "\n",
      "    end_time = time.clock()\n",
      "\n",
      "    training_time = (end_time - start_time)\n",
      "\n",
      "    print >> sys.stderr, ('The 30% corruption code for file ' +\n",
      "                          os.path.split(__file__)[1] +\n",
      "                          ' ran for %.2fm' % (training_time / 60.))\n",
      "\n",
      "    image = PIL.Image.fromarray(tile_raster_images(\n",
      "        X=da.W.get_value(borrow=True).T,\n",
      "        img_shape=(28, 28), tile_shape=(10, 10),\n",
      "        tile_spacing=(1, 1)))\n",
      "    image.save('filters_corruption_30.png')\n",
      "\n",
      "    os.chdir('../')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_dA()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'mnist.pkl'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-50-9b8c23bf0387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_dA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-49-ec0beaad7a22>\u001b[0m in \u001b[0;36mtest_dA\u001b[0;34m(learning_rate, training_epochs, dataset, batch_size, output_folder)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mnist.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'mnist.pkl'"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Can we build a stacked Denoising Autoencoder?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      " This tutorial introduces stacked denoising auto-encoders (SdA) using Theano.\n",
      "\n",
      " Denoising autoencoders are the building blocks for SdA.\n",
      " They are based on auto-encoders as the ones used in Bengio et al. 2007.\n",
      " An autoencoder takes an input x and first maps it to a hidden representation\n",
      " y = f_{\\theta}(x) = s(Wx+b), parameterized by \\theta={W,b}. The resulting\n",
      " latent representation y is then mapped back to a \"reconstructed\" vector\n",
      " z \\in [0,1]^d in input space z = g_{\\theta'}(y) = s(W'y + b').  The weight\n",
      " matrix W' can optionally be constrained such that W' = W^T, in which case\n",
      " the autoencoder is said to have tied weights. The network is trained such\n",
      " that to minimize the reconstruction error (the error between x and z).\n",
      "\n",
      " For the denosing autoencoder, during training, first x is corrupted into\n",
      " \\tilde{x}, where \\tilde{x} is a partially destroyed version of x by means\n",
      " of a stochastic mapping. Afterwards y is computed as before (using\n",
      " \\tilde{x}), y = s(W\\tilde{x} + b) and z as s(W'y + b'). The reconstruction\n",
      " error is now measured between z and the uncorrupted input x, which is\n",
      " computed as the cross-entropy :\n",
      "      - \\sum_{k=1}^d[ x_k \\log z_k + (1-x_k) \\log( 1-z_k)]\n",
      "\n",
      "\n",
      " References :\n",
      "   - P. Vincent, H. Larochelle, Y. Bengio, P.A. Manzagol: Extracting and\n",
      "   Composing Robust Features with Denoising Autoencoders, ICML'08, 1096-1103,\n",
      "   2008\n",
      "   - Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle: Greedy Layer-Wise\n",
      "   Training of Deep Networks, Advances in Neural Information Processing\n",
      "   Systems 19, 2007\n",
      "\n",
      "\"\"\"\n",
      "import cPickle\n",
      "import gzip\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "from theano.tensor.shared_randomstreams import RandomStreams\n",
      "\n",
      "from logistic_sgd import LogisticRegression, load_data\n",
      "from mlp import HiddenLayer\n",
      "from dA import dA\n",
      "\n",
      "\n",
      "class SdA(object):\n",
      "    \"\"\"Stacked denoising auto-encoder class (SdA)\n",
      "\n",
      "    A stacked denoising autoencoder model is obtained by stacking several\n",
      "    dAs. The hidden layer of the dA at layer `i` becomes the input of\n",
      "    the dA at layer `i+1`. The first layer dA gets as input the input of\n",
      "    the SdA, and the hidden layer of the last dA represents the output.\n",
      "    Note that after pretraining, the SdA is dealt with as a normal MLP,\n",
      "    the dAs are only used to initialize the weights.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, numpy_rng, theano_rng=None, n_ins=784,\n",
      "                 hidden_layers_sizes=[500, 500], n_outs=10,\n",
      "                 corruption_levels=[0.1, 0.1]):\n",
      "        \"\"\" This class is made to support a variable number of layers.\n",
      "\n",
      "        :type numpy_rng: numpy.random.RandomState\n",
      "        :param numpy_rng: numpy random number generator used to draw initial\n",
      "                    weights\n",
      "\n",
      "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
      "        :param theano_rng: Theano random generator; if None is given one is\n",
      "                           generated based on a seed drawn from `rng`\n",
      "\n",
      "        :type n_ins: int\n",
      "        :param n_ins: dimension of the input to the sdA\n",
      "\n",
      "        :type n_layers_sizes: list of ints\n",
      "        :param n_layers_sizes: intermediate layers size, must contain\n",
      "                               at least one value\n",
      "\n",
      "        :type n_outs: int\n",
      "        :param n_outs: dimension of the output of the network\n",
      "\n",
      "        :type corruption_levels: list of float\n",
      "        :param corruption_levels: amount of corruption to use for each\n",
      "                                  layer\n",
      "        \"\"\"\n",
      "\n",
      "        self.sigmoid_layers = []\n",
      "        self.dA_layers = []\n",
      "        self.params = []\n",
      "        self.n_layers = len(hidden_layers_sizes)\n",
      "\n",
      "        assert self.n_layers > 0\n",
      "\n",
      "        if not theano_rng:\n",
      "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
      "        # allocate symbolic variables for the data\n",
      "        self.x = T.matrix('x')  # the data is presented as rasterized images\n",
      "        self.y = T.ivector('y')  # the labels are presented as 1D vector of\n",
      "                                 # [int] labels\n",
      "\n",
      "        # The SdA is an MLP, for which all weights of intermediate layers\n",
      "        # are shared with a different denoising autoencoders\n",
      "        # We will first construct the SdA as a deep multilayer perceptron,\n",
      "        # and when constructing each sigmoidal layer we also construct a\n",
      "        # denoising autoencoder that shares weights with that layer\n",
      "        # During pretraining we will train these autoencoders (which will\n",
      "        # lead to chainging the weights of the MLP as well)\n",
      "        # During finetunining we will finish training the SdA by doing\n",
      "        # stochastich gradient descent on the MLP\n",
      "\n",
      "        for i in xrange(self.n_layers):\n",
      "            # construct the sigmoidal layer\n",
      "\n",
      "            # the size of the input is either the number of hidden units of\n",
      "            # the layer below or the input size if we are on the first layer\n",
      "            if i == 0:\n",
      "                input_size = n_ins\n",
      "            else:\n",
      "                input_size = hidden_layers_sizes[i - 1]\n",
      "\n",
      "            # the input to this layer is either the activation of the hidden\n",
      "            # layer below or the input of the SdA if you are on the first\n",
      "            # layer\n",
      "            if i == 0:\n",
      "                layer_input = self.x\n",
      "            else:\n",
      "                layer_input = self.sigmoid_layers[-1].output\n",
      "\n",
      "            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n",
      "                                        input=layer_input,\n",
      "                                        n_in=input_size,\n",
      "                                        n_out=hidden_layers_sizes[i],\n",
      "                                        activation=T.nnet.sigmoid)\n",
      "            # add the layer to our list of layers\n",
      "            self.sigmoid_layers.append(sigmoid_layer)\n",
      "            # its arguably a philosophical question...\n",
      "            # but we are going to only declare that the parameters of the\n",
      "            # sigmoid_layers are parameters of the StackedDAA\n",
      "            # the visible biases in the dA are parameters of those\n",
      "            # dA, but not the SdA\n",
      "            self.params.extend(sigmoid_layer.params)\n",
      "\n",
      "            # Construct a denoising autoencoder that shared weights with this\n",
      "            # layer\n",
      "            dA_layer = dA(numpy_rng=numpy_rng,\n",
      "                          theano_rng=theano_rng,\n",
      "                          input=layer_input,\n",
      "                          n_visible=input_size,\n",
      "                          n_hidden=hidden_layers_sizes[i],\n",
      "                          W=sigmoid_layer.W,\n",
      "                          bhid=sigmoid_layer.b)\n",
      "            self.dA_layers.append(dA_layer)\n",
      "\n",
      "        # We now need to add a logistic layer on top of the MLP\n",
      "        self.logLayer = LogisticRegression(\n",
      "                         input=self.sigmoid_layers[-1].output,\n",
      "                         n_in=hidden_layers_sizes[-1], n_out=n_outs)\n",
      "\n",
      "        self.params.extend(self.logLayer.params)\n",
      "        # construct a function that implements one step of finetunining\n",
      "\n",
      "        # compute the cost for second phase of training,\n",
      "        # defined as the negative log likelihood\n",
      "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n",
      "        # compute the gradients with respect to the model parameters\n",
      "        # symbolic variable that points to the number of errors made on the\n",
      "        # minibatch given by self.x and self.y\n",
      "        self.errors = self.logLayer.errors(self.y)\n",
      "\n",
      "    def pretraining_functions(self, train_set_x, batch_size):\n",
      "        ''' Generates a list of functions, each of them implementing one\n",
      "        step in trainnig the dA corresponding to the layer with same index.\n",
      "        The function will require as input the minibatch index, and to train\n",
      "        a dA you just need to iterate, calling the corresponding function on\n",
      "        all minibatch indexes.\n",
      "\n",
      "        :type train_set_x: theano.tensor.TensorType\n",
      "        :param train_set_x: Shared variable that contains all datapoints used\n",
      "                            for training the dA\n",
      "\n",
      "        :type batch_size: int\n",
      "        :param batch_size: size of a [mini]batch\n",
      "\n",
      "        :type learning_rate: float\n",
      "        :param learning_rate: learning rate used during training for any of\n",
      "                              the dA layers\n",
      "        '''\n",
      "\n",
      "        # index to a [mini]batch\n",
      "        index = T.lscalar('index')  # index to a minibatch\n",
      "        corruption_level = T.scalar('corruption')  # % of corruption to use\n",
      "        learning_rate = T.scalar('lr')  # learning rate to use\n",
      "        # number of batches\n",
      "        n_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "        # begining of a batch, given `index`\n",
      "        batch_begin = index * batch_size\n",
      "        # ending of a batch given `index`\n",
      "        batch_end = batch_begin + batch_size\n",
      "\n",
      "        pretrain_fns = []\n",
      "        for dA in self.dA_layers:\n",
      "            # get the cost and the updates list\n",
      "            cost, updates = dA.get_cost_updates(corruption_level,\n",
      "                                                learning_rate)\n",
      "            # compile the theano function\n",
      "            fn = theano.function(inputs=[index,\n",
      "                              theano.Param(corruption_level, default=0.2),\n",
      "                              theano.Param(learning_rate, default=0.1)],\n",
      "                                 outputs=cost,\n",
      "                                 updates=updates,\n",
      "                                 givens={self.x: train_set_x[batch_begin:\n",
      "                                                             batch_end]})\n",
      "            # append `fn` to the list of functions\n",
      "            pretrain_fns.append(fn)\n",
      "\n",
      "        return pretrain_fns\n",
      "\n",
      "    def build_finetune_functions(self, datasets, batch_size, learning_rate):\n",
      "        '''Generates a function `train` that implements one step of\n",
      "        finetuning, a function `validate` that computes the error on\n",
      "        a batch from the validation set, and a function `test` that\n",
      "        computes the error on a batch from the testing set\n",
      "\n",
      "        :type datasets: list of pairs of theano.tensor.TensorType\n",
      "        :param datasets: It is a list that contain all the datasets;\n",
      "                         the has to contain three pairs, `train`,\n",
      "                         `valid`, `test` in this order, where each pair\n",
      "                         is formed of two Theano variables, one for the\n",
      "                         datapoints, the other for the labels\n",
      "\n",
      "        :type batch_size: int\n",
      "        :param batch_size: size of a minibatch\n",
      "\n",
      "        :type learning_rate: float\n",
      "        :param learning_rate: learning rate used during finetune stage\n",
      "        '''\n",
      "\n",
      "        (train_set_x, train_set_y) = datasets[0]\n",
      "        (valid_set_x, valid_set_y) = datasets[1]\n",
      "        (test_set_x, test_set_y) = datasets[2]\n",
      "\n",
      "        # compute number of minibatches for training, validation and testing\n",
      "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
      "        n_valid_batches /= batch_size\n",
      "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
      "        n_test_batches /= batch_size\n",
      "\n",
      "        index = T.lscalar('index')  # index to a [mini]batch\n",
      "\n",
      "        # compute the gradients with respect to the model parameters\n",
      "        gparams = T.grad(self.finetune_cost, self.params)\n",
      "\n",
      "        # compute list of fine-tuning updates\n",
      "        updates = []\n",
      "        for param, gparam in zip(self.params, gparams):\n",
      "            updates.append((param, param - gparam * learning_rate))\n",
      "\n",
      "        train_fn = theano.function(inputs=[index],\n",
      "              outputs=self.finetune_cost,\n",
      "              updates=updates,\n",
      "              givens={\n",
      "                self.x: train_set_x[index * batch_size:\n",
      "                                    (index + 1) * batch_size],\n",
      "                self.y: train_set_y[index * batch_size:\n",
      "                                    (index + 1) * batch_size]},\n",
      "              name='train')\n",
      "\n",
      "        test_score_i = theano.function([index], self.errors,\n",
      "                 givens={\n",
      "                   self.x: test_set_x[index * batch_size:\n",
      "                                      (index + 1) * batch_size],\n",
      "                   self.y: test_set_y[index * batch_size:\n",
      "                                      (index + 1) * batch_size]},\n",
      "                      name='test')\n",
      "\n",
      "        valid_score_i = theano.function([index], self.errors,\n",
      "              givens={\n",
      "                 self.x: valid_set_x[index * batch_size:\n",
      "                                     (index + 1) * batch_size],\n",
      "                 self.y: valid_set_y[index * batch_size:\n",
      "                                     (index + 1) * batch_size]},\n",
      "                      name='valid')\n",
      "\n",
      "        # Create a function that scans the entire validation set\n",
      "        def valid_score():\n",
      "            return [valid_score_i(i) for i in xrange(n_valid_batches)]\n",
      "\n",
      "        # Create a function that scans the entire test set\n",
      "        def test_score():\n",
      "            return [test_score_i(i) for i in xrange(n_test_batches)]\n",
      "\n",
      "        return train_fn, valid_score, test_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_SdA(finetune_lr=0.1, pretraining_epochs=15,\n",
      "             pretrain_lr=0.001, training_epochs=1000,\n",
      "             dataset='mnist.pkl.gz', batch_size=1):\n",
      "    \"\"\"\n",
      "    Demonstrates how to train and test a stochastic denoising autoencoder.\n",
      "\n",
      "    This is demonstrated on MNIST.\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used in the finetune stage\n",
      "    (factor for the stochastic gradient)\n",
      "\n",
      "    :type pretraining_epochs: int\n",
      "    :param pretraining_epochs: number of epoch to do pretraining\n",
      "\n",
      "    :type pretrain_lr: float\n",
      "    :param pretrain_lr: learning rate to be used during pre-training\n",
      "\n",
      "    :type n_iter: int\n",
      "    :param n_iter: maximal number of iterations ot run the optimizer\n",
      "\n",
      "    :type dataset: string\n",
      "    :param dataset: path the the pickled dataset\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    datasets = load_data(dataset)\n",
      "\n",
      "    train_set_x, train_set_y = datasets[0]\n",
      "    valid_set_x, valid_set_y = datasets[1]\n",
      "    test_set_x, test_set_y = datasets[2]\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
      "    n_train_batches /= batch_size\n",
      "\n",
      "    # numpy random generator\n",
      "    numpy_rng = numpy.random.RandomState(89677)\n",
      "    print '... building the model'\n",
      "    # construct the stacked denoising autoencoder class\n",
      "    sda = SdA(numpy_rng=numpy_rng, n_ins=28 * 28,\n",
      "              hidden_layers_sizes=[1000, 1000, 1000],\n",
      "              n_outs=10)\n",
      "\n",
      "    #########################\n",
      "    # PRETRAINING THE MODEL #\n",
      "    #########################\n",
      "    print '... getting the pretraining functions'\n",
      "    pretraining_fns = sda.pretraining_functions(train_set_x=train_set_x,\n",
      "                                                batch_size=batch_size)\n",
      "\n",
      "    print '... pre-training the model'\n",
      "    start_time = time.clock()\n",
      "    ## Pre-train layer-wise\n",
      "    corruption_levels = [.1, .2, .3]\n",
      "    for i in xrange(sda.n_layers):\n",
      "        # go through pretraining epochs\n",
      "        for epoch in xrange(pretraining_epochs):\n",
      "            # go through the training set\n",
      "            c = []\n",
      "            for batch_index in xrange(n_train_batches):\n",
      "                c.append(pretraining_fns[i](index=batch_index,\n",
      "                         corruption=corruption_levels[i],\n",
      "                         lr=pretrain_lr))\n",
      "            print 'Pre-training layer %i, epoch %d, cost ' % (i, epoch),\n",
      "            print numpy.mean(c)\n",
      "\n",
      "    end_time = time.clock()\n",
      "\n",
      "    print >> sys.stderr, ('The pretraining code for file ' +\n",
      "                          os.path.split(__file__)[1] +\n",
      "                          ' ran for %.2fm' % ((end_time - start_time) / 60.))\n",
      "\n",
      "    ########################\n",
      "    # FINETUNING THE MODEL #\n",
      "    ########################\n",
      "\n",
      "    # get the training, validation and testing function for the model\n",
      "    print '... getting the finetuning functions'\n",
      "    train_fn, validate_model, test_model = sda.build_finetune_functions(\n",
      "                datasets=datasets, batch_size=batch_size,\n",
      "                learning_rate=finetune_lr)\n",
      "\n",
      "    print '... finetunning the model'\n",
      "    # early-stopping parameters\n",
      "    patience = 10 * n_train_batches  # look as this many examples regardless\n",
      "    patience_increase = 2.  # wait this much longer when a new best is\n",
      "                            # found\n",
      "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
      "                                   # considered significant\n",
      "    validation_frequency = min(n_train_batches, patience / 2)\n",
      "                                  # go through this many\n",
      "                                  # minibatche before checking the network\n",
      "                                  # on the validation set; in this case we\n",
      "                                  # check every epoch\n",
      "\n",
      "    best_params = None\n",
      "    best_validation_loss = numpy.inf\n",
      "    test_score = 0.\n",
      "    start_time = time.clock()\n",
      "\n",
      "    done_looping = False\n",
      "    epoch = 0\n",
      "\n",
      "    while (epoch < training_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            minibatch_avg_cost = train_fn(minibatch_index)\n",
      "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
      "\n",
      "            if (iter + 1) % validation_frequency == 0:\n",
      "                validation_losses = validate_model()\n",
      "                this_validation_loss = numpy.mean(validation_losses)\n",
      "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
      "                      (epoch, minibatch_index + 1, n_train_batches,\n",
      "                       this_validation_loss * 100.))\n",
      "\n",
      "                # if we got the best validation score until now\n",
      "                if this_validation_loss < best_validation_loss:\n",
      "\n",
      "                    #improve patience if loss improvement is good enough\n",
      "                    if (this_validation_loss < best_validation_loss *\n",
      "                        improvement_threshold):\n",
      "                        patience = max(patience, iter * patience_increase)\n",
      "\n",
      "                    # save best validation score and iteration number\n",
      "                    best_validation_loss = this_validation_loss\n",
      "                    best_iter = iter\n",
      "\n",
      "                    # test it on the test set\n",
      "                    test_losses = test_model()\n",
      "                    test_score = numpy.mean(test_losses)\n",
      "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
      "                           'best model %f %%') %\n",
      "                          (epoch, minibatch_index + 1, n_train_batches,\n",
      "                           test_score * 100.))\n",
      "\n",
      "            if patience <= iter:\n",
      "                done_looping = True\n",
      "                break\n",
      "\n",
      "    end_time = time.clock()\n",
      "    print(('Optimization complete with best validation score of %f %%,'\n",
      "           'with test performance %f %%') %\n",
      "                 (best_validation_loss * 100., test_score * 100.))\n",
      "    print >> sys.stderr, ('The training code for file ' +\n",
      "                          os.path.split(__file__)[1] +\n",
      "                          ' ran for %.2fm' % ((end_time - start_time) / 60.))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_SdA()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Now... what is all this about \"Generative Models\" ???!??"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is difficult to optimize the weights in\n",
      "nonlinear autoencoders that have multiple\n",
      "hidden layers (2\u20134). With large initial weights,\n",
      "autoencoders typically find poor local minima;\n",
      "with small initial weights, the gradients in the\n",
      "early layers are tiny, making it infeasible to\n",
      "train autoencoders with many hidden layers. If\n",
      "the initial weights are close to a good solution,\n",
      "gradient descent works well, but finding such\n",
      "initial weights requires a very different type of\n",
      "algorithm that learns one layer of features at a\n",
      "time."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Restricted Boltzmann Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The foundation of Restricted Boltzmann Machines goes back to Hopfield Networks. The most commonly known RBM is Bernoulli-Bernoulli RBM (shown below) of which neurons can only take binary values ( 0 or 1). Similar to Sparse Auto-encoders, it learns latent features h to reconstruct the input vector. But instead, the weights w between the input, output , and hidden neurons are shared. Therefore, it is a generative model , ascribing a probability for each observable data that it might generate given a set of latent features. Its objective is to have latent features that are robust enough to reconstruct the sensory input."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/rbm.jpg\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/rbm.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An ensemble of binary vectors (e.g., images)\n",
      "can be modeled using a two-layer network\n",
      "called a restricted Boltzmann machine, in which stochastic, binary pixels\n",
      "are connected to stochastic, binary feature\n",
      "detectors using symmetrically weighted connections.\n",
      "The pixels correspond to \"visible\"\n",
      "units of the RBM because their states are\n",
      "observed; the feature detectors correspond to\n",
      "\"hidden\" units."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A joint configuration (v,h) of the visible and hidden units has an energy given by:\n",
      "\n",
      "$E(\\textbf{v},\\textbf{h}) = - \\sum_{i \\in pixels} b_iv_i - \\sum_{j \\in features} b_jh_j - \\sum_{i,j}v_ih_jw_{ij}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where $v_i$ and $h_j$ are the binary states of pixel $i$ and feature $j$, $b_i$ and $b_j$ are their biases, and $w_ij$ is the weight between them."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The network assigns a probability to every possible image via this energy function."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The probability of a training image can be raised by adjusting the weights and biases to lower the energy of that image and to raise the energy of the similar \"confabulated\" images that the network would prefer to the real data.\n",
      "\n",
      "Given a training image, the binary state $h_j$ of each feature detector $j$ is set to 1 with probability $\\sigma(b_j + \\sum_i v_iw_{ij})$, where $\\sigma(x)$ is the logistic function\n",
      "\n",
      "$\\frac{1}{1+exp(-x)}$\n",
      "\n",
      "$b_j$ is the bias of $j$, $v_i$ is the state of pixel $i$, and $w_{ij}$ is the weight between $i$ and $j$.\n",
      "\n",
      "Once binary states have been chosen for the hidden units, a \"confabulation\" is produced by setting each $v_i$ to 1 with probability $\\sigma(b_i + \\sum_j h_jw_{ij})$, where $b_i$ is the bias of $i$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is saying that due to the affine form of the energy equation, with respect to h, we readily obtain a tractable expression for the conditional probability $P(h|x)$:\n",
      "\n",
      "$P(h|x) = \\frac{exp(b'x +c'h +h'Wx)}{\\sum_{\\tilde{h}}exp(b'x +c'\\tilde{h}+\\tilde{h}'Wx)}$\n",
      "\n",
      "$=\\frac{\\Pi_i exp(c_ih_i +h_iW_ix)}{\\Pi_i \\sum_{\\tilde{h}}exp(c_i\\tilde{h}_i+\\tilde{h}_i'W_ix)}$\n",
      "\n",
      "$= \\Pi_i \\frac{exp(h_i(c_i +W_ix))}{\\sum_{\\tilde{h}_i} exp(\\tilde{h}_i (c_i +W_ix))}$\n",
      "\n",
      "$= \\Pi_i P(h_i|x)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And -thus- in the commonly studied case, where $h_i \\in (0,1)$, we obtain the usual neuron equation for a neuron's output given its input:\n",
      "\n",
      "$P(h_i =1 | x) = \\frac{e^{c_i+W_ix}}{1+e^{c_i+W_ix}} = sigm(c_i +W_ix)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, since x and h play a symmetric role in the energy function, a similar derivation allows to efficiently compute and sample $P(x|h)$:\n",
      "\n",
      "P(x|h) = $\\Pi_i P(x_i|h)$\n",
      "\n",
      "and in the binary case:\n",
      "\n",
      "$P(x_i =1 |h) = sigm(b_j +W_{\\cdot j}'h)$,  where $W_{\\cdot j}$ is the j-th column of W."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The states of the hidden units are then updated once more so that they represent features of the confabulation. The change in weight is given by \n",
      "\n",
      "$\\Delta$$w_{ij} = \\epsilon\\left( <v_ih_j>_{data} - <v_ih_j>_{recon}\\right)$\n",
      "\n",
      "where $\\epsilon$ is a learning rate, $<v_ih_j>_{data}$ is the fraction of times that the pixel $i$ and feature detector $j$ are on together when the feature detectors are being driven by data, and $<v_ih_j>_{recon}$ is the corresponding fraction for confabulations."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A simplified version of the same learning rule is used for the biases.  \n",
      "\n",
      "*note: this is not exactly following the gradient of the log probability of the training data."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "sampling from an RBM:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Samples of p(x) can be obtained by running a Markov chain to convergence, using Gibbs sampling as the\n",
      "transition operator."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gibbs sampling of the joint of N random variables $S = (S_1,..., S_N)$ is done through a sequence of N\n",
      "sampling sub-steps of the form $S_i \\sim p(S_i|S_{-i})$, where $S_{-i}$ contains the N - 1 other random variables in $S$\n",
      "excluding $S_i$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For RBMs, $S$ consists of the set of visible and hidden units. However, since they are conditionally independent,\n",
      "one can perform block Gibbs sampling. In this setting, visible units are sampled simultaneously given\n",
      "fixed values of the hidden units. Similarly, hidden units are sampled simultaneously given the visibles. A\n",
      "step in the Markov chain is thus taken as follows:\n",
      "\n",
      "$h^{(n+1)} \\sim sigm(W'v^{(n)} + c)$\n",
      "\n",
      "$v^{(n+1)} \\sim sigm(Wh^{(n+1)}+b)$\n",
      "\n",
      "where $h^{(n)}$ refers to the set of all hidden units at the n-th step of the Markov chain. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What it means is that, for\n",
      "example, $h_i^{(n+1)}$ is randomly chosen to be 1 (versus 0) with probability $sigm(W_i'v^{(n)} + c_i)$, and similarly,\n",
      "$v^{(n+1)}_j$ is randomly chosen to be 1 (versus 0) with probability $sigm(W_jh^{(n+1)} + b_j)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Graphically, this can be illustrated:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/gibbs.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As $t \\rightarrow \\infty$, samples $(v^{(t)}, h^{(t)})$ are guaranteed to be accurate samples of $p(v, h)$.\n",
      "\n",
      "In theory, each parameter update in the learning process would require running one such chain to convergence.\n",
      "It is needless to say that doing so would be prohibitively expensive. As such, several algorithms have\n",
      "been devised for RBMs, in order to efficiently sample from $p(v, h)$ during the learning process."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "now... let's make it deep!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/stacked_rbm.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After learning one layer of feature detectors,\n",
      "we can treat their activities\u2014when they\n",
      "are being driven by the data\u2014as data for\n",
      "learning a second layer of features. The first\n",
      "layer of feature detectors then become the\n",
      "visible units for learning the next RBM. This\n",
      "layer-by-layer learning can be repeated as many times as desired. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It can be shown that adding an\n",
      "extra layer always improves a lower bound on\n",
      "the log probability that the model assigns to the\n",
      "training data, provided the number of feature\n",
      "detectors per layer does not decrease and their\n",
      "weights are initialized correctly (G. E. Hinton, S. Osindero, Y. W. Teh, Neural Comput. 18,\n",
      "1527 (2006).). This bound\n",
      "does not apply when the higher layers have\n",
      "fewer feature detectors."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " This layer-by-layer\n",
      "learning algorithm is nonetheless a very effective\n",
      "way to pretrain the weights of a deep autoencoder.\n",
      "Each layer of features captures strong,\n",
      "high-order correlations between the activities of\n",
      "units in the layer below. For a wide variety of\n",
      "data sets, this is an efficient way to progressively\n",
      "reveal low-dimensional, nonlinear\n",
      "structure."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "now...unroll it:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After pretraining multiple layers of feature\n",
      "detectors, the model is unfolded (below) to\n",
      "produce encoder and decoder networks that\n",
      "initially use the same weights. The global finetuning\n",
      "stage then replaces stochastic activities\n",
      "by deterministic, real-valued probabilities and\n",
      "uses backpropagation through the whole autoencoder\n",
      "to fine-tune the weights for optimal\n",
      "reconstruction."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/unroll.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, in Hinton et al. (2006), binomial inputs are used to encode pixel gray levels as if they were the probability\n",
      "of a binary event. In the case of handwritten character images this approximation works well, but in other\n",
      "cases it does not. Experiments showing the advantage of using Gaussian input units rather than binomial\n",
      "units when the inputs are continuous-valued are described in Bengio et al. (2007). See Welling et al. (2005)\n",
      "for a general formulation where x and h (given the other) can be in any of the exponential family distributions\n",
      "(discrete and continuous).\n",
      "\n",
      "Although RBMs might not be able to represent efficiently some distributions that could be represented\n",
      "compactly with an unrestricted Boltzmann machine, RBMs can represent any discrete distribution (Freund\n",
      "& Haussler, 1994; Le Roux & Bengio, 2008), if enough hidden units are used. In addition, it can be shown\n",
      "that unless the RBM already perfectly models the training distribution, adding a hidden unit (and properly\n",
      "choosing its weights and bias) can always improve the log-likelihood (Le Roux & Bengio, 2008).\n",
      "\n",
      "An RBM can also be seen as forming a multi-clustering. Each\n",
      "hidden unit creates a 2-region partition of the input space (with a linear separation). The binary setting of\n",
      "the hidden units identifies one region in input space among all the regions associated with configurations of\n",
      "the hidden units. Note that not all configurations of the hidden units correspond to a non-empty region in\n",
      "input space. This representation is similar to what an ensemble of 2-leaf trees would create."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hinton says:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"It has been obvious since the 1980s that\n",
      "backpropagation through deep autoencoders\n",
      "would be very effective for nonlinear dimensionality\n",
      "reduction, provided that computers\n",
      "were fast enough, data sets were big enough,\n",
      "and the initial weights were close enough to a\n",
      "good solution. All three conditions are now\n",
      "satisfied. Unlike nonparametric methods,\n",
      "autoencoders give mappings in both directions\n",
      "between the data and code spaces, and they can\n",
      "be applied to very large data sets because both\n",
      "the pretraining and the fine-tuning scale linearly\n",
      "in time and space with the number of training\n",
      "cases.\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}